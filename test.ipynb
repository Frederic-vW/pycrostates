{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_clusters = 5\n",
    "labels = np.random.randint(0, n_clusters, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "# code for the PAIF manuscript:\n",
    "# F. von Wegner, Partial Autoinformation to Characterize Symbolic Sequences\n",
    "# Front Physiol (2018) https://doi.org/10.3389/fphys.2018.01382\n",
    "# FvW 05/2018, Python3 version 05/2021\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- define logarithm\n",
    "#log = np.log\n",
    "log = np.log2\n",
    "\n",
    "\n",
    "\"\"\"*****************************************************************************\n",
    "!!! ALL SYMBOLS HAVE TO BE INTEGERS: 0, 1, 2, ... !!!\n",
    "*****************************************************************************\"\"\"\n",
    "\n",
    "\n",
    "def H_1(x, ns):\n",
    "    \"\"\"\n",
    "    Shannon entropy of the symbolic sequence x with ns symbols.\n",
    "\n",
    "    Args:\n",
    "        x: symbolic sequence, symbols = [0, 1, ..., ns-1]\n",
    "        ns: number of symbols\n",
    "    Returns:\n",
    "        h: Shannon entropy of x\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(x)\n",
    "    p = np.zeros(ns)  # symbol distribution\n",
    "    for t in range(n):\n",
    "        p[x[t]] += 1.0\n",
    "    p /= n\n",
    "    h = -np.sum(p[p>0]*log(p[p>0]))\n",
    "    return h\n",
    "\n",
    "\n",
    "def H_2(x, y, ns):\n",
    "    \"\"\"\n",
    "    Joint Shannon entropy of the symbolic sequences x, y, with ns symbols.\n",
    "\n",
    "    Args:\n",
    "        x, y: symbolic sequences, symbols = [0, 1, ..., ns-1]\n",
    "        ns: number of symbols\n",
    "    Returns:\n",
    "        h: joint Shannon entropy of (x, y)\n",
    "    \"\"\"\n",
    "\n",
    "    if (len(x) != len(y)):\n",
    "        print(\"H_2 warning : sequences of different lengths, using the shorter...\")\n",
    "    n = min(len(x), len(y))\n",
    "    p = np.zeros((ns, ns)) # joint distribution\n",
    "    for t in range(n):\n",
    "        p[x[t],y[t]] += 1.0\n",
    "    p /= n\n",
    "    h = -np.sum(p[p>0]*log(p[p>0]))\n",
    "    return h\n",
    "\n",
    "\n",
    "def H_k(x, ns, k):\n",
    "    \"\"\"\n",
    "    Joint Shannon entropy of k-histories x[t:t+k]\n",
    "\n",
    "    Args:\n",
    "        x: symbolic sequence, symbols = [0, 1, ..., ns-1]\n",
    "        ns: number of symbols\n",
    "        k: length of k-history\n",
    "    Returns:\n",
    "        h: joint Shannon entropy of x[t:t+k]\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(x)\n",
    "    p = np.zeros(tuple(k*[ns]))  # symbol joint distribution\n",
    "    for t in range(n-k):\n",
    "        p[tuple(x[t:t+k])] += 1.0\n",
    "    p /= (n-k) # normalize distribution\n",
    "    h = -np.sum(p[p>0]*log(p[p>0]))\n",
    "    #m = np.sum(p>0)\n",
    "    #h = h + (m-1)/(2*N) # Miller-Madow bias correction\n",
    "    return h\n",
    "\n",
    "def H_k_fix(x, ns, k):\n",
    "    \"\"\"\n",
    "    Joint Shannon entropy of k-histories x[t:t+k]\n",
    "\n",
    "    Args:\n",
    "        x: symbolic sequence, symbols = [0, 1, ..., ns-1]\n",
    "        ns: number of symbols\n",
    "        k: length of k-history\n",
    "    Returns:\n",
    "        h: joint Shannon entropy of x[t:t+k]\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(x)\n",
    "    p = np.zeros(tuple(k*[ns]))  # symbol joint distribution\n",
    "    for t in range(n-k+1):\n",
    "        p[tuple(x[t:t+k])] += 1.0\n",
    "    p /= (n-k+1) # normalize distribution\n",
    "    h = -np.sum(p[p>0]*log(p[p>0]))\n",
    "    #m = np.sum(p>0)\n",
    "    #h = h + (m-1)/(2*N) # Miller-Madow bias correction\n",
    "    return h\n",
    "\n",
    "def ais(x, ns, k=1):\n",
    "    \"\"\"\n",
    "    Active information storage (AIS)\n",
    "\n",
    "    TeX notation:\n",
    "    I(X_{n+1} ; X_{n}^{(k)})\n",
    "    = H(X_{n+1}) - H(X_{n+1} | X_{n}^{(k)})\n",
    "    = H(X_{n+1}) - H(X_{n+1},X_{n}^{(k)}) + H(X_{n}^{(k)})\n",
    "    = H(X_{n+1}) + H(X_{n}^{(k)}) - H(X_{n+1}^{(k+1)})\n",
    "\n",
    "    Args:\n",
    "        x: symbolic sequence, symbols = [0, 1, ..., ns-1]\n",
    "        ns: number of symbols\n",
    "        k: history length (optional, default value k=1)\n",
    "    Returns:\n",
    "        a: active information storage\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(x)\n",
    "    h1 = H_k(x, ns, 1)\n",
    "    h2 = H_k(x, ns, k)\n",
    "    h3 = H_k(x, ns, k+1)\n",
    "    a = h1 + h2 - h3\n",
    "    return a\n",
    "\n",
    "def ais_fix(x, ns, k=1):\n",
    "    \"\"\"\n",
    "    Active information storage (AIS)\n",
    "\n",
    "    TeX notation:\n",
    "    I(X_{n+1} ; X_{n}^{(k)})\n",
    "    = H(X_{n+1}) - H(X_{n+1} | X_{n}^{(k)})\n",
    "    = H(X_{n+1}) - H(X_{n+1},X_{n}^{(k)}) + H(X_{n}^{(k)})\n",
    "    = H(X_{n+1}) + H(X_{n}^{(k)}) - H(X_{n+1}^{(k+1)})\n",
    "\n",
    "    Args:\n",
    "        x: symbolic sequence, symbols = [0, 1, ..., ns-1]\n",
    "        ns: number of symbols\n",
    "        k: history length (optional, default value k=1)\n",
    "    Returns:\n",
    "        a: active information storage\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(x)\n",
    "    h1 = H_k_fix(x, ns, 1)\n",
    "    h2 = H_k_fix(x, ns, k)\n",
    "    h3 = H_k_fix(x, ns, k+1)\n",
    "    a = h1 + h2 - h3\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2940526443131133\n",
      "2.2940526443131133\n",
      "2.2940526443131133\n"
     ]
    }
   ],
   "source": [
    "# n_clusters doesn't change H1\n",
    "for n in [n_clusters, 10, 12]:\n",
    "    print(H_1(labels, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.506185519664854\n",
      "4.506185519664854\n",
      "4.506185519664854\n"
     ]
    }
   ],
   "source": [
    "# n_clusters doesn't change H_k\n",
    "for n in [n_clusters, 10, 12]:\n",
    "    print(H_k(labels, n, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2940526443131133\n",
      "2.290127190985782\n",
      "2.2940526443131133\n"
     ]
    }
   ],
   "source": [
    "# H_k (k=1)!= H_1\n",
    "print(H_1(labels, n))\n",
    "print(H_k(labels, n, 1))\n",
    "# due to len(labels) - k + 1\n",
    "print(H_k_fix(labels, n, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pycrostates.segmentation.autoinformation import entropy,joint_entropy, joint_entropy_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(labels) == H_1(labels, n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(joint_entropy(labels, labels), H_2(labels, labels, n_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(joint_entropy_history(labels, 3), H_k_fix(labels, n_clusters, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.117903692680288"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_k(labels, n_clusters, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.137619155317624"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycrostates_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
